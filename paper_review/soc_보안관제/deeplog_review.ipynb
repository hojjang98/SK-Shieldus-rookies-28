{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4958d05f",
   "metadata": {},
   "source": [
    "# 📄 DeepLog: Anomaly Detection and Diagnosis from System Logs\n",
    "> ACM CCS 2017 | Min Du, Feifei Li, Guineng Zheng, Vivek Srikumar  \n",
    "> [논문 링크](https://users.cs.utah.edu/~lifeifei/papers/deeplog.pdf)\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Day 1 – Abstract & Introduction\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 연구 배경과 동기\n",
    "- **현대 시스템의 로그 문제**  \n",
    "  대규모 분산 시스템(Hadoop, Spark 등)은 초당 수천 건 이상의 로그를 생성한다. 이 로그는 시스템의 상태와 이상 행동을 파악할 수 있는 핵심 단서이지만, 데이터의 양과 복잡성으로 인해 사람이 직접 분석하기 어렵다.\n",
    "  \n",
    "\n",
    "- **기존 접근의 한계**  \n",
    "  - 규칙 기반 탐지 방식은 미리 정의된 패턴에만 반응하므로 새로운 유형의 이상(New Anomaly)을 탐지하지 못한다.  \n",
    "  - PCA, SVM 등 통계적·전통적 머신러닝 기법은 로그의 **순차적 의존성(sequence dependency)** 을 반영하지 못한다.  \n",
    "\n",
    "\n",
    "- **핵심 연구 질문**  \n",
    "  “로그를 언어 시퀀스로 모델링하여,  \n",
    "  다음에 발생할 이벤트를 예측함으로써 이상을 탐지할 수 있을까?”\n",
    "  \n",
    "<br>\n",
    "\n",
    "### 📌 핵심 아이디어\n",
    "- 로그를 **문장(Sequence)** 으로, 각 로그 이벤트를 **단어(Token)** 로 간주한다.  \n",
    "- LSTM(Long Short-Term Memory) 구조를 이용해 **정상 시퀀스의 다음 이벤트를 예측**한다.  \n",
    "- 예측 결과와 실제 이벤트가 일치하지 않으면 이를 **이상(Anomaly)** 으로 판단한다.  \n",
    "- 학습은 정상 로그만으로 이루어지며, **One-class Supervised Learning** 형태로 동작한다.\n",
    "\n",
    "<br>\n",
    "\n",
    "### 📌 주요 기여\n",
    "1. **시퀀스 기반 이상탐지 프레임워크 제안**  \n",
    "   로그를 시계열 문맥으로 해석하고 LSTM으로 다음 이벤트를 예측하는 방법을 제시했다.  \n",
    "2. **자동 진단(Diagnosis) 기능 제공** 이상이 발생하면 관련 로그 시퀀스를 역추적하여 원인을 분석할 수 있다.  \n",
    "3. **성능 향상 입증**  \n",
    "   PCA, Invariant Mining 등 기존 방식보다 정밀도와 재현율이 모두 향상됨을 보였다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "### 📌 핵심 인사이트\n",
    "- 로그는 단순한 시스템 출력이 아니라, **시스템의 언어**이다.  \n",
    "- DeepLog는 이 언어의 패턴을 학습하여 시스템의 정상 행동을 이해하고,그로부터 벗어나는 패턴을 이상으로 탐지한다.  \n",
    "- 이는 규칙을 정의하는 기존 관점에서 벗어나,**정상의 맥락(Context)을 학습하는 관제 방식**으로의 전환을 의미한다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "### 🧠 개인적 해석\n",
    "- DeepLog는 “정상을 정의하면 이상은 자연히 드러난다”는 철학을 잘 보여준다.  \n",
    "- SOC 환경에서 딥러닝을 적용한 초기 사례로서, **‘로그 = 언어’라는 개념적 전환점**을 마련했다.  \n",
    "- 이러한 시퀀스 기반 접근은 이후 Transformer 구조를 활용한 LogBERT, LogGPT 등의 후속 연구로 발전하는 출발점이 되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a888e85",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ✅ Day 2 – Related Work & Framework Overview\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 관련 연구 (Related Work)\n",
    "\n",
    "- **규칙 기반 / 통계 기반 접근의 한계**\n",
    "  - 전통적으로 로그 이상탐지는 **정의된 규칙(rule)**, **정상 분포 기반 모델**, 또는 **PCA 기반 차원축소** 등을 활용했다.  \n",
    "  - 하지만 이런 방법들은 **패턴 변화에 취약**하고, **로그 간 시간적·의존 관계**를 고려하지 못한다.\n",
    "  - 예: `Event A → Event B → Event C` 가 정상이라면, `Event A → Event D` 같은 새로운 조합은 감지 불가능하다.\n",
    "\n",
    "\n",
    "- **머신러닝 기반 접근**\n",
    "  - SVM, Isolation Forest 등 일부 ML 기법도 사용되었지만,  \n",
    "    로그가 갖는 **순차적 구조(sequence dependency)** 를 명시적으로 모델링하지 못한다.\n",
    "  - 결국 로그는 “독립된 점들의 집합”으로 처리되어, 맥락 이해가 불가능했다.\n",
    "\n",
    "\n",
    "- **DeepLog의 새로운 시도**\n",
    "  - DeepLog는 로그를 **자연어 문장처럼 모델링**하여,  \n",
    "    **다음 이벤트 예측(next event prediction)** 문제로 변환한다.  \n",
    "  - 이를 통해 시간 순서, 이벤트 간 전이 패턴, 반복 구조 등을 학습할 수 있다.\n",
    "  \n",
    "<br>\n",
    "\n",
    "### 📌 전체 구조 (Framework Overview)\n",
    "\n",
    "\n",
    "#### 1️⃣ 로그 전처리 단계 (Log Parsing)\n",
    "\n",
    "<div style=\"background-color:#eef6ff; padding:12px 15px; border-left:5px solid #4A90E2; border-radius:6px;\">\n",
    "INFO Block 1234 received from node 10.1.1.5  \n",
    "→ Block <*> received from node <*>  \n",
    "→ Event ID = 45  \n",
    "</div>\n",
    "\n",
    "\n",
    "- 로그는 원래 **비정형 문자열**이므로, 이를 **Log Key** 단위로 변환한다.  \n",
    "- 위와 같은 과정을 통해 로그를 **토큰화된 시퀀스(event ID sequence)** 로 표현할 수 있다.  \n",
    "- 일반적으로 **Drain**, **Spell** 같은 로그 파서(parser)를 사용해 정규화한다.  \n",
    "\n",
    "\n",
    "#### 2️⃣ 시퀀스 모델 학습 (Sequence Modeling)\n",
    "\n",
    "- DeepLog는 로그 시퀀스를 **LSTM(Long Short-Term Memory)** 기반 모델에 입력하여 다음에 발생할 이벤트를 예측한다.  \n",
    "- 모델은 최근 **W개의 이벤트 윈도우(window)** 를 관찰하고, 그 문맥으로부터 다음 이벤트를 예측한다.\n",
    "\n",
    "$$\n",
    "P(x_{t+1} | x_t, x_{t-1}, \\ldots, x_{t-W+1})\n",
    "$$\n",
    "\n",
    "- 학습은 **정상 로그 데이터만**을 이용하며,이는 비정상 레이블이 없는 환경에서의 **One-class Supervised Training** 형태이다.  \n",
    "\n",
    "\n",
    "#### 3️⃣ 이상 탐지 (Anomaly Detection)\n",
    "\n",
    "- 실제 로그 스트림에서 다음 이벤트가 발생했을 때,  \n",
    "  모델이 예측한 상위 Top-k 이벤트 목록에 없다면 → **이상(Anomaly)** 으로 판단한다.\n",
    "  \n",
    "<div style=\"background-color:#eef6ff; padding:12px 15px; border-left:5px solid #4A90E2; border-radius:6px;\">\n",
    "Input sequence: [E1, E2, E3, E4]  \n",
    "Predicted next events: [E5, E6, E7]  \n",
    "Actual next event: E9  \n",
    "→ E9 ∉ Top-k → **Anomaly Detected** \n",
    "</div>\n",
    "\n",
    "- 이런 방식은 새로운 이벤트 조합이나 이전에 없던 패턴도 감지할 수 있게 한다.  \n",
    "\n",
    "\n",
    "#### 4️⃣ 진단 단계 (Diagnosis)\n",
    "\n",
    "- 이상이 감지되면, DeepLog는 해당 시퀀스의 **hidden state 변화**를 분석하여 이상이 발생한 위치(원인 이벤트)를 추정한다.  \n",
    "- 즉, 단순 탐지뿐 아니라 **원인 규명(root-cause analysis)** 도 수행할 수 있다. \n",
    "\n",
    "<div style=\"background-color:#eef6ff; padding:12px 15px; border-left:5px solid #4A90E2; border-radius:6px;\">\n",
    "예시:  \n",
    "정상: A → B → C → D  \n",
    "이상:  A → B → X → D\n",
    "</div>\n",
    "\n",
    "→ Hidden state 변화가 X에서 급격함 → X가 root cause로 추정 \n",
    "\n",
    "- 실제 논문에서는 LSTM의 내부 상태를 시각화하여 정상/비정상 시퀀스의 차이를 명확히 보여주었다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "### 📌 핵심 인사이트\n",
    "\n",
    "- 로그를 **시간 순서가 있는 언어 시퀀스**로 해석한 것이 핵심 혁신이다.  \n",
    "- LSTM은 시퀀스 내의 **장기 의존성(Long-term dependency)** 을 학습하여 시스템의 “정상 동작 패턴”을 내재적으로 파악한다.  \n",
    "- 이상 탐지는 더 이상 “규칙 위반”이 아니라, **예측 불일치(prediction inconsistency)** 로 정의된다.  \n",
    "\n",
    "<br>\n",
    "\n",
    "### 🧠 개인적 해석\n",
    "\n",
    "- DeepLog의 파이프라인은 NLP의 “다음 단어 예측(next-token prediction)” 구조와 동일하다.  \n",
    "- 정상 로그만으로 학습한다는 점에서, **One-Class Learning** 혹은 **Self-Supervised Prediction**의 초기 형태라 할 수 있다.  \n",
    "- “Top-k 예측 안에 없으면 이상”이라는 간단한 규칙은 실제 시스템 환경에서도 구현이 간단하고 효과적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a344c4a",
   "metadata": {},
   "source": [
    "---\n",
    "## ✅ Day 3 – Model Details & Training Process\n",
    "---\n",
    "\n",
    "### 📌 1️⃣ 입력 데이터 구조 (Input Representation)\n",
    "\n",
    "- DeepLog는 로그를 **이벤트 ID 시퀀스**로 변환한 후, 최근 `w`개의 로그를 입력으로 사용한다.  \n",
    "- 각 로그 이벤트는 **임베딩 벡터(embedding vector)** 로 매핑된다.  \n",
    "  (단순한 정수 ID가 아니라, 이벤트 간 유사도를 학습할 수 있는 벡터 표현이다.)  \n",
    "\n",
    "입력 시퀀스 예시:  \n",
    "<div style=\"background-color:#eef6ff; padding:12px 15px; border-left:5px solid #4A90E2; border-radius:6px;\">\n",
    "x_t = [e_{t-w+1}, e_{t-w+2}, ..., e_t]\n",
    "</div>\n",
    "- 임베딩 차원(dimension)은 약 128~256 수준으로 설정하며, 학습 중 자동으로 업데이트된다.  \n",
    "- 입력 윈도우 크기(`w`)는 로그 패턴의 길이에 따라 조정하며, 논문에서는 `w = 10`이 최적이었다.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 2️⃣ LSTM 네트워크 구조 (Network Architecture)\n",
    "\n",
    "- LSTM은 순차 데이터의 시간적 의존성을 학습하기 위한 신경망이다.  \n",
    "- 각 시간 단계에서, LSTM은 현재 이벤트의 임베딩(`e_t`)과 이전 hidden state(`h_{t-1}`)를 받아 새로운 상태(`h_t`)를 계산한다.\n",
    "\n",
    "간단한 형태로 표현하면:\n",
    "<div style=\"background-color:#eef6ff; padding:12px 15px; border-left:5px solid #4A90E2; border-radius:6px;\">\n",
    "h_t = LSTM(e_t, h_{t-1})\n",
    "</div>\n",
    "\n",
    "- 마지막 hidden state `h_t`는 fully connected layer에 전달되어 다음 이벤트의 확률 분포를 예측한다.  \n",
    "\n",
    "예측 단계:\n",
    "<div style=\"background-color:#eef6ff; padding:12px 15px; border-left:5px solid #4A90E2; border-radius:6px;\">  \n",
    "P(x_{t+1}) = Softmax(W * h_t + b)\n",
    "</div>\n",
    "\n",
    "- 여기서 `W`, `b`는 학습 가능한 파라미터이며, Softmax를 통해 모든 이벤트 ID에 대한 확률을 구한다.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 3️⃣ 학습 과정 (Training Phase)\n",
    "\n",
    "- 학습 데이터는 **정상 로그 시퀀스만**으로 구성된다.  \n",
    "- 학습 목표는 “다음 이벤트를 맞추는 것”이며, 손실 함수는 **Cross-Entropy Loss**이다.\n",
    "\n",
    "<div style=\"background-color:#eef6ff; padding:12px 15px; border-left:5px solid #4A90E2; border-radius:6px;\"> \n",
    "Loss = - Σ ( y_i * log(ŷ_i) )\n",
    "</div>\n",
    "\n",
    "\n",
    "- `y_i`: 실제 다음 이벤트의 one-hot 벡터  \n",
    "- `ŷ_i`: 모델이 예측한 확률 분포  \n",
    "\n",
    "**학습 설정 (논문 기준):**\n",
    "- Optimizer: Adam  \n",
    "- Learning rate: 0.001  \n",
    "- Batch size: 128  \n",
    "- Epochs: 약 50~100  \n",
    "- Early Stopping 적용  \n",
    "\n",
    "---\n",
    "\n",
    "### 📌 4️⃣ 이상 탐지 로직 (Detection Logic)\n",
    "\n",
    "- 예측 시, 모델은 다음 이벤트의 확률 상위 `K`개를 계산한다.  \n",
    "- 실제 발생한 이벤트가 그 안에 포함되지 않으면 → **이상(Anomaly)** 으로 판단한다.\n",
    "\n",
    "예시:  \n",
    "<div style=\"background-color:#eef6ff; padding:12px 15px; border-left:5px solid #4A90E2; border-radius:6px;\"> \n",
    "입력 시퀀스: [E1, E2, E3, E4]\n",
    "모델 예측 Top-3: [E5, E6, E7]\n",
    "실제 이벤트: E9\n",
    "→ E9 not in Top-3 → Anomaly Detected\n",
    "</div>\n",
    "\n",
    "- `K` 값은 정밀도(Precision)와 재현율(Recall)의 균형을 조정하는 파라미터이며,  \n",
    "  논문에서는 `K = 9`일 때 가장 우수한 성능을 보였다.\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 5️⃣ 이상 진단 (Anomaly Diagnosis)\n",
    "\n",
    "- LSTM의 hidden state 변화량을 분석하면 **이상의 원인 이벤트(root-cause)** 를 추정할 수 있다.  \n",
    "- hidden state 간 거리(예: cosine distance)가 갑자기 커지는 지점이 비정상 행동의 원인이다.\n",
    "\n",
    "예시:  \n",
    "<div style=\"background-color:#eef6ff; padding:12px 15px; border-left:5px solid #4A90E2; border-radius:6px;\"> \n",
    "정상 시퀀스: A → B → C → D\n",
    "이상 시퀀스: A → B → X → D\n",
    "</div>\n",
    "→ hidden state 변화가 X에서 급격함 → X가 root cause로 추정됨\n",
    "\n",
    "---\n",
    "\n",
    "### 📊 주요 하이퍼파라미터 요약\n",
    "\n",
    "| 파라미터 | 설명 | 값 (논문 기준) |\n",
    "|-----------|------|----------------|\n",
    "| Window size (w) | 입력 시퀀스 길이 | 10 |\n",
    "| Hidden size | LSTM 은닉 차원 | 128 |\n",
    "| Embedding size | 이벤트 임베딩 차원 | 128 |\n",
    "| Batch size | 학습 배치 크기 | 128 |\n",
    "| Learning rate | 학습률 | 0.001 |\n",
    "| Optimizer | 가중치 업데이트 방식 | Adam |\n",
    "| Top-K | 이상 탐지 기준 | 9 |\n",
    "\n",
    "---\n",
    "\n",
    "### 💡 핵심 인사이트\n",
    "\n",
    "- DeepLog는 로그를 언어처럼 다루며, 정상 동작의 문맥을 학습한다.  \n",
    "- 정상 로그만으로 시스템의 행동 패턴을 충분히 학습할 수 있음을 보여준다.  \n",
    "- Hidden state를 이용한 진단 기능은 단순 탐지를 넘어,  \n",
    "  **“왜 이상이 발생했는가”** 를 설명할 수 있게 한다.\n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 개인적 해석\n",
    "\n",
    "- DeepLog는 “시스템 로그의 언어모델(System Log Language Model)” 개념의 출발점이다.  \n",
    "- 정상 로그 기반 학습은 실제 SOC 환경(이상 레이블 부족)에 매우 현실적이다.  \n",
    "- hidden state 분석은 보안 관제에서 이상 원인(예: 특정 이벤트 조합)을 시각적으로 파악하는 데 유용하다.  \n",
    "- 오늘날 LogBERT, LogGPT 등의 Transformer 기반 모델은 모두 이 구조를 확장한 형태라 볼 수 있다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ml_env)",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
